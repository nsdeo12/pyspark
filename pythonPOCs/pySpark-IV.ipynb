{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1a85d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EMPLOYEE_ID: integer (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- EMAIL: string (nullable = true)\n",
      " |-- PHONE_NUMBER: string (nullable = true)\n",
      " |-- HIRE_DATE: string (nullable = true)\n",
      " |-- JOB_ID: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      " |-- COMMISSION_PCT: string (nullable = true)\n",
      " |-- MANAGER_ID: string (nullable = true)\n",
      " |-- DEPARTMENT_ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col as pyspark_col\n",
    "spark=SparkSession.builder.appName('testApp').getOrCreate()\n",
    "# df=spark.read.csv(\"s3://citibike-s3test/listingsDataViz.csv\",header=True,inferSchema=True)\n",
    "df=spark.read.csv(\"C:\\\\Users\\\\Nilakantha\\\\Documents\\\\My Folder\\\\pythonPOCs\\\\employees.csv\",header=True,inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4726d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-Jun-07|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-Jan-08|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-Sep-03|   AD_ASST|  4400|            - |       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-Feb-04|    MK_MAN| 13000|            - |       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-Aug-05|    MK_REP|  6000|            - |       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777| 7-Jun-02|    HR_REP|  6500|            - |       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888| 7-Jun-02|    PR_REP| 10000|            - |       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080| 7-Jun-02|    AC_MGR| 12008|            - |       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181| 7-Jun-02|AC_ACCOUNT|  8300|            - |       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17-Jun-03|   AD_PRES| 24000|            - |        - |           90|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'SALARY' given input columns: [MANAGER_ID, avg(SALARY)];\n'Sort ['SALARY ASC NULLS FIRST], true\n+- Aggregate [MANAGER_ID#721], [MANAGER_ID#721, avg(SALARY#719) AS avg(SALARY)#1220]\n   +- Project [SALARY#719, MANAGER_ID#721]\n      +- Relation [EMPLOYEE_ID#712,FIRST_NAME#713,LAST_NAME#714,EMAIL#715,PHONE_NUMBER#716,HIRE_DATE#717,JOB_ID#718,SALARY#719,COMMISSION_PCT#720,MANAGER_ID#721,DEPARTMENT_ID#722] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NILAKA~1\\AppData\\Local\\Temp/ipykernel_78224/331346190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SALARY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MANAGER_ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MANAGER_ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SALARY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36msort\u001b[1;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'SALARY' given input columns: [MANAGER_ID, avg(SALARY)];\n'Sort ['SALARY ASC NULLS FIRST], true\n+- Aggregate [MANAGER_ID#721], [MANAGER_ID#721, avg(SALARY#719) AS avg(SALARY)#1220]\n   +- Project [SALARY#719, MANAGER_ID#721]\n      +- Relation [EMPLOYEE_ID#712,FIRST_NAME#713,LAST_NAME#714,EMAIL#715,PHONE_NUMBER#716,HIRE_DATE#717,JOB_ID#718,SALARY#719,COMMISSION_PCT#720,MANAGER_ID#721,DEPARTMENT_ID#722] csv\n"
     ]
    }
   ],
   "source": [
    "df.show(10)\n",
    "df.select(col(\"SALARY\"),col(\"MANAGER_ID\")).groupBy(\"MANAGER_ID\").mean().orderBy(\"SALARY\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41f1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
